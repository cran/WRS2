%\VignetteIndexEntry{WRS2: Robust Statistical Methods}
%\VignetteEngine{knitr::knitr} 

\documentclass[article, nojss]{jss}
\usepackage{amsmath, amsfonts, thumbpdf}
\usepackage{float,amssymb}
\usepackage{hyperref}
\usepackage{amsmath}

\newcommand{\defi}{\mathop{=}\limits^{\Delta}}  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% declarations for jss.cls %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% almost as usual
\author{Patrick Mair\\ Harvard University \And 
        Rand Wilcox\\ University of Southern California
        }
\title{Robust Statistical Methods: The \proglang{R} Package \pkg{WRS2}}

\Plainauthor{Patrick Mair, Rand Wilcox} %% comma-separated
\Plaintitle{A Collection of Robust Statistical Methods: The R Package WRS2} %% without formatting
\Shorttitle{The \pkg{WRS2} Package} %% a short title (if necessary)

%% an abstract and keywords
\Abstract{
In this manuscript we present various robust statistical methods and show how to apply them in \proglang{R} using the \pkg{WRS2} package. 
We elaborate on robust location measures and present robust $t$-test versions for independent and dependent samples. We focus on robust one-way and higher order ANOVA strategies including mixed designs (``between-within subjects''). Finally, we elaborate on running interval 
smoothers which we use in robust ANCOVA. 
}

\Keywords{\pkg{WRS2}, robust location measures, robust ANOVA, robust ANCOVA}
\Plainkeywords{WRS2, robust location measures, robust ANOVA, robust ANCOVA} 

%% publication information
%% NOTE: This needs to filled out ONLY IF THE PAPER WAS ACCEPTED.
%% If it was not (yet) accepted, leave them commented.
%% \Volume{13}
%% \Issue{9}
%% \Month{September}
%% \Year{2004}
%% \Submitdate{2004-09-29}
%% \Acceptdate{2004-09-29}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
  Patrick Mair\\
  Department of Psychology\\
  Harvard University\\
  E-mail: \email{mair@fas.harvard.edu}\\
  URL: \url{http://http://scholar.harvard.edu/mair}
}
%% It is also possible to add a telephone and fax number
%% before the e-mail in the following format:
%% Telephone: +43/1/31336-5053
%% Fax: +43/1/31336-734

%% for those who use Sweave please include the following line (with % symbols):
%% need no \usepackage{Sweave.sty}

%% end of declarations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}

<<echo=FALSE, message=FALSE, results='hide'>>=
require(WRS2)
require(beanplot)
require(car)
require(MASS)
require(colorspace)
require(reshape)
require(ez)
@
\section{Introduction}
Data are rarely normal. Yet many classical statistical methods assume normally distributed data, especially when it comes to small samples. For large samples the central limit theorem tells us that we do not have to worry too much. Unfortunately, things are a little bit more complex than that when it comes to statistical testing, especially when we have to deal with prominent ``dangerous'' normality deviations such heavily skewed data, data with outliers, and heavy-tailed distribution. 

Before elaborating on consequences of these violations within the context of statistcal testing and estimation, let us look at the impact of normality deviations from a purely descriptive angle. It is common knowledge that the mean can be heavily affected by outliers or highly skewed distributions. Computing the mean on such data would not give us the 
``typical'' participant; it is just not a good location measure to characterize the sample. In this case, once strategy is to use more robust measure such as the median or trimmed mean. Corresponding statistical tests involving such robust parameters are outside the classical statistical 
testing framework, however. Another strategy to deal with such violations (especially right-skewed data) is to apply transformations such as the logarithm or more sophisticated Box-Cox transformations \citep{Box+Cox:1964}. For instance, in a simple $t$-test scenario where we want to compate two group means we can think of applying log-transformations within each group which could make the data ``more normal''. 
The problem with this strategy is that a subsequent $t$-test compares the log-means between the groups (i.e., the geometric means) rather than the original means. This might not be in line anymore with the original research question and hypotheses. 

Apart from such descriptive considerations, violations from normality influence the results of statistical tests. The approximation of sampling distribution of the test statistic might not be proper, test results might be biased, confidence intervals not estimated in a satisfactory manner.
In addition, the power of classical test statistics becomes low. In general, we have the following options when doing inference on small, ugly datasets and we are worried about the outcomes. We can stay within the parametric framework and establish the sampling distribution via permutation strategies. The \proglang{R} \citep{R:2015} package \pkg{coin} \citep{Hothorn:2008} gives a general implementation of basic permutation strategies. Another option is to perform a parametric or nonparametric bootstrap for which the \pkg{boot} package \citep{Canty+Ripley:2015} provides a flexible framework. Alternatively, we can switch into the nonparametric testing world. Nonparametric tests have less restrictive distributional assumptions than their parametric friends. Prominent examples for classical nonparametric tests 
taught in most introductory statistics class are the Mann-Whitney $U$-test \citep{Mann+Whitney:1947}, the Wilcoxon signed-rank and rank-sum test \citep{Wilcoxon:1945}, and Kruskal-Wallis ANOVA \citep{Kruskal+Wallis:1952}. 

Robust methods for statistical estimation and testing provide another great option to deal with data that are not well-behaved. Historically, the first developments can be traced back to the 60's with publications by \citet{Tukey:1960}, \citet{Huber:1964}, and \citet{Hampel:1968}. Measures that characterize a distribution (such as location and scale) are said to be \emph{robust} if slight changes in a distribution have a relatively small effect on their value \citep[][p. 23]{Wilcox:2012}. Robust methods are still assuming a functional form of the probability distribution but the main goal is to produce outcomes that are less sensitive to small departures from the assumed functional form. These methods are important in situations where researchers have a considerably small sample, deviating from normality. In such situations it is not a good idea to apply classical statistical tests such as $t$-tests, ANOVA, ANCOVA, etc. since they may deliver biased results or their power may be low. 

This article introduces the \pkg{WRS2} package that implements methods from the original \pkg{WRS} package (see \url{https://github.com/nicebread/WRS/tree/master/pkg}) in a more user-friendly manner. We focus on basic testing scenarios especially relevant for the social sciences and introduce these methods in a simple way. For further technical and computational details on the original \pkg{WRS} functions as well as additional tests the reader is referred to \citet{Wilcox:2012}. 

Before we elaborate on the \pkg{WRS2} package, let us give an overview of some important robust methods are available in various \proglang{R}. 
packages. In general, \proglang{R} is pretty well endowed with all sorts of robust regression functions and packages such as \code{rlm} in \pkg{MASS} \citep{Venables+Ripley:2002}, \code{lmrob} and \code{nlrob} in \pkg{robustbase} \citep{Rousseeuw:2015}. The latter function performs nonlinear robust regression. Robust mixed-effects models are implemented in \pkg{robustlmm} \citep{Koller:2015} and robust generalized additive models in \pkg{robustgam} \citep{Wong:2014}. Regarding multivariate methods, the \pkg{rrcov} package \citep{Filzmoser:2009} provides various implementations such as robust multivariate variance-covariance estimation and robust PCA. \pkg{FRB} \citep{Aelst:2013} includes bootstrap based approaches for multivariate regression, PCA and Hotelling tests, \pkg{RSKC} \citep{Kondo:2014} functions for robust $k$-means clustering, and \pkg{robustDA} \citep{Bouveyron:2015} performs robust discriminant analysis. Additional packages for robust statistics can be found on the CRAN Task View on robust statistics (URL: \url{https://cran.r-project.org/web/views/Robust.html}). 

\newpage
\section{Robust Measures of Location}
\label{sec:robloc}
A robust alternative to the mean is the \emph{trimmed mean} which discards a certain percentage at both ends of the distribution. For instance, 
a 20\% trimmed mean cuts-off 20\% at the low end and 20\% the high end. In \proglang{R}, a trimmed mean can be computed via the basic \code{mean} function by setting the \code{trim} argument accordingly. Note that if the trimming portion is set to $\gamma = 0.5$, the trimmed mean 
$\bar x_t$ results in the median $\tilde x$.

Another alternative is the \emph{Winsorized mean}. The process of giving less weight to observations in the tails of the distribution and higher weight to the ones in the center, is called \emph{Winsorizing}. Instead of computing the mean on the original distribution we compute the mean on the Winsorized distribution. Similar to the trimmed mean, the amount of Winsorizing has to choosen a priori. The \pkg{WRS2} function to compute Windsorized means is \code{winmean}. 

A general family of robust location measures are $M$\emph{-estimators} (the ``M'' stands for ``maximum likelihood-type''). The basic idea is to define a loss function to be minimized. For instance, if the loss function is $\sum_{i=1}^n (x_i - \mu)^2$, minimization results in the arithmetic mean $\hat{\mu} = \frac{1}{n}\sum_{i=1}^n x_i$. Instead of such a quadratic loss we can think of a more general, differentiable distance function $\xi(\cdot)$: 

\begin{equation}
\sum_{i=1}^n \xi(x_i - \mu_m) \rightarrow \textrm{min!}
\end{equation}

Let $\Psi = \xi'(\cdot)$ denote its derivative. The minimization problem reduces to $\sum_{i=1}^n \Psi(x_i - \mu_m) = 0$ where 
$\mu_m$ denotes the $M$-estimator. Several distance functions have been proposed in the literature. As an example, Huber 
\citep[see][]{Huber:1981} proposed the following function:

\begin{equation}
\Psi(x) = 
\begin{cases}
    x            & \quad \text{if } |x|\leq K\\
    K \text{sign}(x)      & \quad \text{if } |x|> K\\
  \end{cases}
\end{equation}

$K$ is the bending constant for which Huber proposed a value of $K = 1.28$. Increasing $K$ increases efficiency when 
sampling from a normal distribution, but increases sensitivity to the tails of the distribution. 
The estimation of $M$-estimators is performed iteratively and implemented in the \code{mest} function. More details and additional distance functions can be found in \citet{Wilcox:2012}.


% continue p. 52, simulate function (see p. 31) and compute various measures. 
% Chapter 3
% sample trimmed mean (3.3), quantiles (3.5), M-estimators (3.6), outlier detection (3.13)

% describe MOM, onestep and MADN (further below)

\section{Robust $t$-Test and ANOVA Strategies}
In this section these robust location measures are used in order to test for differences across groups. We focus on basic $t$-test strategies
(independent and dependent groups), and various ANOVA approaches including mixed designs (i.e., between-within subjects designs). 

\subsection{Tests on Location Measures for Two Independent Groups}
\citet{Yuen:1974} proposed a test statistic for a two-sample trimmed mean test which allows for unequal variances. The test statistic is given by 
\begin{equation}
\label{eq:yuen}
T_y = \frac{\bar X_{t1} - \bar X_{t2}}{\sqrt{d_1 + d_2}},
\end{equation}
which, under the null ($H_0$: $\mu_{t1} = \mu_{t2}$), follows a $t$-distribution\footnote{It is not suggested to use this test statistic for a $\gamma = 0.5$ trimming level (which would result in median comparisons) since the standard errors become highly inaccurate.}. Details on computation of the standard error and the degrees of freedom can be found in \citet[p. 157--158]{Wilcox:2012}. If no trimming is involved, this method reduces to Welch's classical $t$-test with unequal variances \citep{Welch:1938}. Yuen's test in implemented in the \code{yuen} function. 
There is also a bootstrap version of it (see \code{yuenbt}) which is suggested to use for one-sided testing when the group sample sizes are unequal. 

Let us look at an example. The dataset comprises various soccer team statistics in five different European 
leagues, collected at the end of the 2008/2009 season. For the moment, let us just focus on the Spanish Primera Division (20 teams) and the German 
Bundesliga (18 teams). We are interested in comparing the trimmed means of goals scored per game across these two Leagues. 

The group-wise boxplots and beanplots in Figure~\ref{fig:soccerplot} visualize potential differences in the distributions. 
Spain has a fairly right-skewed goal distribution involving three outliers (Barcelona, Real Madrid, Atletico Madrid). In the German league, things look more balanced and symmetric. Performing a $t$-test on the group means could be risky, since the Spanish mean could be affected by the outliers. A saver way is to perform a test on the trimmed means. We keep the default trimming level of $\gamma = 0.2$.


<<soccer-plot, eval=FALSE, echo = FALSE>>=
SpainGer <- subset(eurosoccer, League == "Spain" | League == "Germany")
SpainGer <- droplevels(SpainGer)
op <- par(mfrow = c(1,2))
boxplot(GoalsGame ~ League, data = SpainGer, main = "Boxplot Goals Scored per Game")
points(1:2, tapply(SpainGer$GoalsGame, SpainGer$League, mean, trim = 0.2), pch = 19, col = "red")
beanplot(GoalsGame ~ League, data = SpainGer, log = "", main = "Beanplot Goals Scored per Game", 
         col = "coral")
par(op)
@

\begin{figure}
\begin{center}  
<<soccer-plot1, echo=FALSE, fig.height = 6, fig.width = 12, dev='postscript'>>=
<<soccer-plot>>
@
\end{center}
\caption{\label{fig:soccerplot} Left panel: boxplots for scored goals per game (Spanish vs. German league). The 
red dots correspond to the 20\% trimmed means. Right panel: beanplots for the same setting.}
\end{figure}
 
Running a two-sample trimmed mean test suggests that there are no significant differences in the trimmed means 
across the two leagues:

<<>>=
yuen(GoalsGame ~ League, data = SpainGer)
@
 
If we want to run a test on median differences, or more general $M$-estimator differences, the \code{pb2gen} function can be used. 
<<>>=
pb2gen(GoalsGame ~ League, data = SpainGer, est = "median")
pb2gen(GoalsGame ~ League, data = SpainGer, est = "onestep")
@

The first test related to median differences, the second test to Huber's $\Psi$ estimator. The results in this 
particular example are consistent for various robust location estimators. 

% effect sizes 
%  binomials (5.8), variances (5.5), dependent groups (5.9.6)

\subsection{One-way Multiple Group Comparisons}
Often it is said that $F$-tests are quite robust against violations. 
This is not always the case. In fact, discussions and examples given in \citet{Games:1984}, \citet{Tan:1982}, \citet{Wilcox:1996} and \citet{Cressie+Whitford:1986} show that things can go wrong when applying ANOVA in situations where we have heavy-tailed distributions, unequal sample sizes, and when distributions differ in skewness. Transforming the data is not a very appealing alternative either since, as in a $t$-test setting, we end up comparing geometric means. 

The first robust alternative present here is a one-way comparison of multiple trimmed group means, implemented in the 
\code{t1way} function. Let $J$ be the number of groups. The corresponding null hypothesis is:

\[
H_0: \mu_{t1} = \mu_{t2} = \cdots = \mu_{tJ}.
\]

The corresponding test statistic which approximates an $F$-distribution under the null, is quite complicated and can be found in 
\citet[p. 293]{Wilcox:2012}. A bootstrap version is provided in \code{t1waybt}. If no trimming is involved we end up with Welch's ANOVA version allowing for unequal variances \citep{Welch:1951}.  

% Maybe a bit more description here, see Wilcox p. 319

A similar test statistic can be derived for comparing medians instead of trimmed means, implemented in the 
\code{med1way} function. Let us apply these two tests on the soccer dataset. This time we include all 
five leagues. Figure~\ref{fig:soccerplot2} shows the corresponding boxplots and beanplots. We see that Germany and 
Italy have a pretty symmetric distribution, England and The Nethderlands right-skewed distributions, and Spain has outliers. 

<<soccer2-plot, eval=FALSE, echo=FALSE>>=
op <- par(mfrow = c(2,1))
boxplot(GoalsGame ~ League, data = eurosoccer, main = "Boxplot Goals Scored per Game")
beanplot(GoalsGame ~ League, data = eurosoccer, log = "", main = "Beanplot Goals Scored per Game", 
         col = "coral")
par(op)
@
\begin{figure}[t]
\begin{center}  
<<soccer2-plot1, echo=FALSE, fig.height = 10, fig.width = 12, dev='postscript'>>=
<<soccer2-plot>>
@
\end{center}
\caption{\label{fig:soccerplot2} Left panel: Boxplots for scored goals per game (Spanish vs. German league). Right panel: Beanplots for the same setting.}
\end{figure}
 
In \pkg{WRS2} these robust one-way ANOVA variants can be computed as follows:

<<>>=
t1way(GoalsGame ~ League, data = eurosoccer)
med1way(GoalsGame ~ League, data = eurosoccer)
@

Again, none of the tests suggests a significant difference in robust location parameters across groups. 
For illustration, let us just perform all pairwise comparisons on the same data setting. Post hoc tests on the trimmed means can be computed using the \code{lincon} function:

<<>>=
lincon(GoalsGame ~ League, data = eurosoccer)
@

Post hoc tests for the bootstrap version of the trimmed mean ANOVA (\code{t1waybt}) are provided in \code{mcppb20}. 

\subsection{Comparisons Involving Higher-Order Designs}
Let us start with two-way factorial ANOVA design involving $J$ categories for the first factor, and $K$ categories 
for the second factor. The test statistic for the one-way trimmed mean comparisons can be easily generalized to two-way 
designs. The corresponding function is called \code{t2way}. Median comparisons can be performed via \code{med2way} whereas 
for more general $M$-estimators, the function \code{pbad2way} does the job. Note that all \pkg{WRS2} robust ANOVA functions fit the full model 
including all possible interactions only.

As an example we use the infamous beer goggles dataset by \citet{Field:2012}. This dataset is about the effects of alcohol on mate selection in night-clubs. The hypothesis is that after alcohol had been consumed, subjective perceptions of physical attractiveness would become more inaccurate (\emph{beer goggles effect}). In this dataset we have the two factors gender (24 male and 24 femals students) and the amount of alcohol consumed (none, 2 pints, 4 pints). At the end of the evening the researcher took a photograph of the person the participant was chatting up. The attractiveness of the person on the photo was then evaluated by independent judges on a scale from 0-100 (response variable). Figure~\ref{fig:goggles} shows the interaction plots using the median as location measure. It looks like there is some interaction going on between gender and the amount of alcohol in terms of attractiveness rating. The following code chunk computes three robust two-way ANOVA 
versions as well as a standard ANOVA for comparison.

<<goggles-plot, eval=FALSE, echo=FALSE>>=
attach(goggles)
op <- par(mfrow = c(1,2))
interaction.plot(gender, alcohol, attractiveness, fun = median, ylab = "Attractiveness", xlab = "Gender", type = "b", pch = 20,
                 lty = 1, col = c("black", "cadetblue", "coral"), main = "Interaction Plot Alcohol|Gender", legend = FALSE)  
legend("right", legend = c("None", "2 Pints","4 Pints"), col = c("black", "cadetblue", "coral"), lty = 1, cex = 0.8)
interaction.plot(alcohol, gender, attractiveness, fun = median, ylab = "Attractiveness", xlab = "Gender", type = "b", pch = 20,
                 lty = 1, col = c("coral", "black"), main = "Interaction Plot Gender|Alcohol", legend = FALSE) 
legend("bottomleft", legend = c("female", "male"), col = c("coral", "black"), lty = 1, cex = 0.8)
par(op)
detach(goggles)
@

\begin{figure}[t]
\begin{center}  
<<goggles-plot1, echo=FALSE, fig.height = 6, fig.width = 12, dev='postscript'>>=
<<goggles-plot>>
@
\end{center}
\caption{\label{fig:goggles} Interaction plot involving the median attractiveness ratings in beer goggles dataset.}
\end{figure}

<<echo=3:6>>=
set.seed(123)
goggles$alcohol <- relevel(goggles$alcohol, ref = "None") #FIXME
t2way(attractiveness ~ gender*alcohol, data = goggles)
med2way(attractiveness ~ gender*alcohol, data = goggles)
pbad2way(attractiveness ~ gender*alcohol, data = goggles, est = "onestep")
summary(aov(attractiveness ~ gender*alcohol, data = goggles))
@

In each case we get a significant interaction. Going back to the interaction plots in Figure~\ref{fig:goggles} we see that the attractiveness 
of the date drops significantly for the males if they had four pints. If we are interested in post hoc comparisons, \pkg{WRS2} provides functions 
for the trimmed mean version (\code{mcp2atm}) and the $M$-estimator version (\code{mcp2a}). Here we give the results for the trimmed mean 
version:

<<>>=
mcp2atm(attractiveness ~ gender*alcohol, data = goggles)
@

The most interesting post hoc result is the \code{gender1:alcohol3} contrast which explains the striking 4 pint attractiveness 
drop for the males. 

Now we move on to higher-order designs. \pkg{WRS2} provides the function \code{t3way} for robust three-way ANOVA based on trimmed means. 
The dataset we use is from \citet{Seligman:1990}. At a swimming team practice, 58 participants were asked to swim their best event as far as possible, but in each case the time that was reported was falsified to indicate poorer than expected performance (i.e., each swimmer was disappointed). 30 min later, they did the same performance. The authors predicted that on the second trial more pessimistic swimmers would do worse than on their first trial, whereas optimistic swimmers would do better. The response is $\text{ratio = Time1/Time2}$. A ratio larger than 1 means that a swimmer performed better in trial 2. Figure~\ref{fig:swim} shows two separate interaction plots for male and female swimmers, involving the 20\% trimmed means. 


<<swim-plot, echo=FALSE, eval=FALSE>>=
tmean20 <- function(x) mean(x, trim = 0.20)
optpes.male <- subset(swimming, Sex == "Male")
optpes.female <- subset(swimming, Sex == "Female")
op <- par(mfrow = c(1,2))
interaction.plot(optpes.male$Event, optpes.male$Optim, optpes.male$Ratio, fun = tmean20, 
                 xlab = "Event", ylab = "Ratio", main = "Interaction Men", 
                 type = "b", pch = 20, lty = 1, col = 1:2, legend = FALSE)
legend("topleft", legend = c("Optimists", "Pessimists"), col = 1:2, lty = 1, cex = 0.8)
interaction.plot(optpes.female$Event, optpes.female$Optim, optpes.female$Ratio, fun = tmean20,
                 xlab = "Event", ylab = "Ratio", main = "Interaction Women", 
                 type = "b", pch = 20, lty = 1, col = 1:2, legend = FALSE)
legend("topleft", legend = c("Optimists", "Pessimists"), col = 1:2, lty = 1, cex = 0.8)
par(op)
@

\begin{figure}[t]
\begin{center}  
<<swim-plot1, echo=FALSE, fig.height = 6, fig.width = 12, dev='postscript'>>=
<<swim-plot>>
@
\end{center}
\caption{\label{fig:swim} Interaction plot involving the trimmed means of the time ratio response for males and females separately.}
\end{figure}

Now we compute a three-way robust ANOVA on the trimmed means. For comparison, we also fit a standard three-way ANOVA (since the 
design is unbalanced we print out the Type II sum-of-squares). 

<<>>=
t3way(Ratio ~ Optim*Sex*Event, data = swimming)
fitaov_op <- aov(Ratio ~ Optim*Sex*Event, data = swimming)  
Anova(fitaov_op, type = "II")       
@

The crucial effect is the \code{Optim:Sex} two-way interaction. Figure~\ref{fig:swim2} shows the two-way interaction plot, ignoring the 
swimming style effect. These plots suggests that, if the swimming style is ignored, for the females it does not matter whether someone is an optimist or a pessimist. For the males, there is a significant difference in the time ratio for optimists and pessimists. 

<<swim2-plot, echo=FALSE, eval=FALSE>>=
op <- par(mfrow = c(1,2))
interaction.plot(swimming$Optim, swimming$Sex, swimming$Ratio, fun = tmean20, 
                 xlab = "Optimist/Pessimist", ylab = "Ratio", main = "Interaction Sex|Optim", 
                 type = "b", pch = 19, lty = 1, col = 1:2, legend = FALSE)
legend("bottomleft", legend = c("Male", "Female"), col = 1:2, lty = 1, cex = 0.8)
interaction.plot(swimming$Sex, swimming$Optim, swimming$Ratio, fun = tmean20, 
                 xlab = "Sex", ylab = "Ratio", main = "Interaction Optim|Sex", 
                 type = "b", pch = 19, lty = 1, col = 1:2, legend = FALSE)
legend("bottomright", legend = c("Optimists", "Pessimists"), col = 1:2, lty = 1, cex = 0.8)
par(op)
@

\begin{figure}[t]
\begin{center}  
<<swim2-plot1, echo=FALSE, fig.height = 6, fig.width = 12, dev='postscript'>>=
<<swim2-plot>>
@
\end{center}
\caption{\label{fig:swim2} Interaction plot involving the trimmed means of the time ratio response for gender and optimists/pessimists
(swimming style ignored).}
\end{figure}


\subsection{Repeated Measurement Designs}
\label{sec:rmd}
The simplest repeated measurement design is a paired samples $t$-test scenario. Yuen's trimmed mean $t$-test in Equation~(\ref{eq:yuen}) 
can be generalized to

\begin{equation}
\label{eq:yuen2}
T_y = \frac{\bar X_{t1} - \bar X_{t2}}{\sqrt{d_1 + d_2 - 2d_{12}}}.
\end{equation}

Expressions for the standard deviations can be found in \citet[p. 196]{Wilcox:2012}. The corresponding \proglang{R} function is called 
\code{yuend}. The dataset we use for illustration is in the \pkg{MASS} package and presents data pairs involving weights of girls before and after treatment for anorexia. We use a subset of 17 girls subject to family treatment. 

Figure~\ref{fig:ano} presents the individual trajectories. We see that for four girls the treatment did not seem to be effective, for the remaining ones we have an increase in weight. The paired samples test on the trimmed mean differences gives a significant treatment effect which tells us that, overall, the treatment was effective.

<<>>=
anorexiaFT <- subset(anorexia, subset = Treat == "FT")
yuend(anorexiaFT$Prewt, anorexiaFT$Postwt)
@


<<ano-plot, echo=FALSE, eval=FALSE>>=
colpal <- c(rainbow_hcl(17, c = 50))
matplot(t(anorexiaFT[,2:3]), type = "b", cex = 0.8, main = "Weight Trajectories", 
        xaxt = "n", ylab = "Weight (lbs.)", lty = 1, col = colpal, pch = 20)
axis(1, at = 1:2, labels = c("Prior", "Post"))
@

\begin{figure}[t]
\begin{center}  %, 
<<ano-plot1, echo=FALSE, fig.height = 5, fig.width = 8, dev='postscript'>>=
<<ano-plot>>
@
\end{center}
\caption{\label{fig:ano} Individual weight trajectories of anorexic girls before and after treatment.}
\end{figure}

Let us extend this setting to more than two dependent categories. The \pkg{WRS2} package provides a robust implementation of a heteroscedastic repeated measurement ANOVA based on the trimmed means. The main function is \code{rmanova} with corresponding post hoc tests in \code{rmmcp}. The bootstrap version of \code{rmanova} is \code{rmanovab} with bootstrap post hocs in \code{pairdepb}. 

Each function for robust repeated measurement ANOVA takes three arguments; the data need to be in long format: a vector with the responses (argument: \code{y}), a factor for the groups (e.g,. time points; argument: \code{groups}), and a factor for the blocks (typically a subject ID; argument: \code{blocks}). The data we use to illustrate the functions is a hypothetical wine tasting dataset. There are three types of wine (A, B and C). 22 people tasted each of the three wines (in a blind fold fashion), five times each. The response reflects the average ratings for each wine. Thus, each of the three wines gets one score from each rater. In total, we have 66 scores. The trajectories are given in 
Figure~\ref{fig:wine}. 

<<wine-plot, echo=FALSE, eval=FALSE>>=
WineTasting_wide <- reshape(WineTasting, idvar = "Taster", timevar = "Wine", direction = "wide")[-1]  ## wide format
colpal <- c(rainbow_hcl(22, c = 100))
#pal <- palette(colpal)
matplot(t(WineTasting_wide), pch = 20, type = "b", xaxt = "n", xlab = "Wines", ylab = "Score", lty = 1, col = colpal, main = "Wine Trajectories")
axis(1, at = 1:3, labels = levels(WineTasting$Wine))
#palette(pal)
@

\begin{figure}[t]
\begin{center}   
<<wine-plot1, echo=FALSE, fig.height = 5, fig.width = 8, dev='postscript'>>=
<<wine-plot>>
@
\end{center}
\caption{\label{fig:wine} 22 taster trajectories for three different wines.}
\end{figure}

A robust dependent samples ANOVA on the trimmed means can be fitted as follows: 

<<echo=2:3>>=
attach(WineTasting)
rmanova(y = Taste, groups = Wine, block = Taster)
rmmcp(y = Taste, groups = Wine, block = Taster)
detach(WineTasting)
@

We see that we have a somewhat contradictory result: the global test tells us that there are no significant differences between the wines, whereas the post hoc tests suggest significant differences for the Wine C contrasts. Such results sometimes occur in small sample ANOVA applications when the global test statistic is close to the critical value.


\subsection{Mixed Designs}
This subsection deals with mixed ANOVA designs, that is, we have within-subjects effects (e.g., due to repeated measurements) and 
between-subjects effects (group comparisons). For the parameteric case, the standard \code{aov} function in \proglang{R} is 
able to handle such scenarios, even though in a very limited way. The \code{ezANOVA} function 
in the \pkg{ez} package \citep{Lawrence:2013} allows for an easy specification of such models and also provides some permutation options via \code{ezPerm}. Since such designs belong to the mixed-effects model family, standard packages like \pkg{lme4} \citep{Bates:2015} 
or \pkg{nlme} \citep{Pinheiro:2015} can be applied which provide a great deal of modeling flexibility.  

The main function in \pkg{WRS2} for computing a between-within subjects ANOVA on the trimmed means is \code{bwtrim}. For general 
$M$-estimators, the package offers the bootstrap based functions \code{sppba}, \code{sppbb}, and \code{sppbi} for the between-subjects effect, the within-subjects effect, and the interaction effect, respectively. Each of these functions requires the full model specification through the \code{formula} interface as well as an \code{id} argument that accounts for the within-subject structure. 

The example we use is from \citet[p. 411]{Wilcox:2012}. In a study on the effect of consuming alcohol, the number hangover symptoms were measured for two independent groups, with each subject consuming alcohol and being measured on three different occasions. One group consisted of sons of alcoholics and the other was a control group. A representation of the dataset is given in Figure~\ref{fig:hang}.

<<hang-plot, eval=FALSE, echo=FALSE>>=
ind <- rep(1:6, each = 20)
symlist <- split(hangover$symptoms, ind)[c(1,4,2,5,3,6)]
gtmeans <- sapply(symlist, mean, trim = 0.2)
plot(1:3, type = "n", ylim = c(0, max(hangover$symptoms) + 10), xaxt = "n", xlab = "Time Points", 
     ylab = "Number of Symptoms", main = "Hangover Data")
axis(1, at = 1:3, labels = paste("Time", 1:3))
for (i in 1:6) points(jitter(rep(ceiling(i/2), 20)), symlist[[i]], cex = 0.6, col = ((i %% 2) + 1))
legend("topleft", legend = c("control", "alcoholic"), lty = 1, col = 1:2)
lines(1:3, gtmeans[c(1, 3, 5)], col = 1, type = "b", pch = 19)
lines(1:3, gtmeans[c(2, 4, 6)], col = 2, type = "b", pch = 19)
@

\begin{figure}[t]
\begin{center}   
<<hang-plot1, echo=FALSE, fig.height = 5, fig.width = 8, dev='postscript'>>=
<<hang-plot>>
@
\end{center}
\caption{\label{fig:hang} 20\% trimmed means of the number of hangover symptoms across three time points.}
\end{figure}

First, we fit the between-within subjects ANOVA on the 20\% trimmed means:

<<>>=
bwtrim(symptoms ~ group*time, id = id, data = hangover)
@

We get significant group and time effects. Second, we fit a standard between-within subjects ANOVA through \code{bwtrim} by setting the trimming level to 0. For comparison we fit the same model through \code{ezANOVA} and see that both functions lead to the same results.

<<warning=FALSE>>=
bwtrim(symptoms ~ group*time, id = id, data = hangover, tr = 0)
fitF <- ezANOVA(hangover, symptoms, between = group, within = time, wid = id)
fitF$ANOVA
@

Finally, we base our comparisons on Huber's $M$-estimator for which we have to apply three separate functions, one for each effect. 

<<echo=2:4>>=
set.seed(123)
sppba(symptoms ~ group*time, id, data = hangover)
sppbb(symptoms ~ group*time, id, data = hangover)
sppbi(symptoms ~ group*time, id, data = hangover)
@

These tests give us a significant group effect whereas the time and interaction effects are not significant. 


\section{Robust Nonparametric ANCOVA}
\subsection{Running Interval Smoothers}
Before we talk about robust ANCOVA, we need to do some elaborations on smoothers. In general, a smoother is a function that approximates the data points while leaving out noise in the data. Smoothing functions typically have a smoothing parameter by which the user can steer the 
degree of smoothing. If the parameter is too small, the smoothing function might overfit the data. If the parameter is too large, we might 
disregard important patterns. The general strategy is to find the smallest parameter so that the plot looks reasonably smooth. 

A popular regression smoother is LOWESS (locally weighted scatterplot smoothing) regression which belongs to the family of nonparametric 
regression models and can be fitted using the \code{lowess} function. The smoothers presneted here involve robust
location measures from Section \ref{sec:robloc} and are called \emph{running interval smoothers}. 

Let us start with the trimmed mean. We have pairs of observations ($x_i$, $y_i$). The strategy behind an interval smoother is to compute the $\gamma$-trimmed mean using all of the $y_i$ values for which the corresponding $x_i$'s are close to a value of interest $x$ \citep[][p. 562]{Wilcox:2012}. Let MAD be the median absolute deviation, i.e., $\text{MAD} = \text{median}|x_i - \widetilde{x}|$. Let $\text{MADN} = \text{MAD}/z_{0.75}$, where $z_{0.75}$ represents the quantile of the standard normal distribution. The point $x$ is said to be close to $x_i$ if 

\[
|x_i - x| \leq f \times \text{MADN}.
\]

Here, $f$ as a constant which will turn out to be the smoothing parameter. As $f$ increases, the neighborhood of $x$ gets larger. Let 

\[
N(x_i) = \{j: |x_j-x_i| \leq f \times \text{MADN}\}
\]

such that $N(x_i)$ indexes all the $x_j$ values that are close to $x_i$. Let $\hat{\theta}_i$ be a robust location parameter of interest. A running interval smoother computes $n$ $\hat{\theta}_i$ parameters based on the corresponding $y$-value for which 
$x_j$ is close to $x_i$, that is, the smoother defines an interval and runs across all the $x$-values. Within a regression context, these estimates represent the fitted values. Eventually, we can plot the $(x_i, \hat{\theta}_i)$ tuples into the $(x_i, y_i)$ scatterplot which 
gives us the nonparametric regression fit. The smoothness of this function depends on $f$. 

The \pkg{WRS2} package provides smoothers for trimmed means (\code{runmean}), general $M$-estimators (\code{rungen}), and 
bagging versions of general $M$-estimators (\code{runmbo}), recommended for small datasets. Let us look at a data example, involving various $f$ values and various robust location measures $\hat{\theta}_i$. We use a simple dataset from \cite{Wright+London:2009} where we are interested whether the length and heat of a chile are related. The length was measured in centimeters, the heat on a scale from 0 (``for sissys'') to 11 (``nuclear''). 

<<smooth-plot, eval=FALSE, echo=FALSE>>=
colpal <- c(rainbow_hcl(5, c = 100))
pal <- palette(colpal)
attach(chile)
op <- par(mfrow = c(1,2))
plot(length, heat, pch = 20, col = "gray", main = "Chile Smoothing I", xlab = "Length", ylab = "Heat")
fitmean <- runmean(length, heat)
fitmest <- rungen(length, heat)
fitmed <- rungen(length, heat, est = "median")
fitbag <- runmbo(length, heat, est = "onestep")
orderx <- order(length)
lines(length[orderx], fitmean[orderx], lwd = 2, col = 1)
lines(length[orderx], fitmest[orderx], lwd = 2, col = 2)
lines(length[orderx], fitmed[orderx], lwd = 2, col = 3)
lines(length[orderx], fitbag[orderx], lwd = 2, col = 4)
legend("topright", legend = c("Trimmed Mean", "MOM", "Median", "Bagged Onestep"), col = 1:4, lty = 1)
plot(length, heat, pch = 20, col = "gray", main = "Chile Smoothing II", xlab = "Length", ylab = "Heat")
fitmean1 <- runmean(length, heat, fr = 0.2)
fitmean2 <- runmean(length, heat, fr = 0.5)
fitmean3 <- runmean(length, heat, fr = 1)
fitmean4 <- runmean(length, heat, fr = 5)
orderx <- order(length)
lines(length[orderx], fitmean1[orderx], lwd = 2, col = 1)
lines(length[orderx], fitmean2[orderx], lwd = 2, col = 2)
lines(length[orderx], fitmean3[orderx], lwd = 2, col = 3)
lines(length[orderx], fitmean4[orderx], lwd = 2, col = 4)
legend("topright", legend = c("f = 0.2", "f = 0.5", "f = 1", "f = 5"), col = 1:4, lty = 1)
par(op)
palette(pal)
detach(chile)
@
\begin{figure}[t]
\begin{center}  
<<smooth-plot1, echo=FALSE, fig.height = 6, fig.width = 12, dev='postscript'>>=
<<smooth-plot>>
@
\end{center}
\caption{\label{fig:smooth} Left panel: smoothers with various robust location measures. Right panel: trimmed mean smoother with 
varying smoothing parameter $f$.}
\end{figure}

The left panel in Figure~\ref{fig:smooth} displays smoothers involving different robust location measures. The right panel shows  
a trimmed mean interval smoothing with varying smoothing parameter $f$. We see that, at least in this dataset, there are no striking 
differences between the smoothers with varying location measure. The choice of the smoothing parameter $f$ affects the function 
heavily, however. 

\subsection{Robust ANCOVA}
ANCOVA involves a factorial design and metric covariates that were not part of the experimental manipulation. Basic ANCOVA assumes 
homogeneity of regression slopes across the groups when regressing the dependent variable on the covariate. A further assumption is homoscedasticity of the error terms across groups. The robust ANCOVA function in \pkg{WRS2} does not assume homoscedasticity nor homogeneity of regression 
slopes. In fact, it does not make any parametric assumption on the regressions at all and uses running interval smoothing (trimmed means) for each subgroup. Both nonparametric curves can be tested for subgroup differences at various points of interest along the $x$-continuum. This makes 
it very similar to what \emph{functional data analysis} \citep[FDA; see][]{Ramsay+Silverman:2005} is doing. The main difference is that FDA uses 
smoothing splines whereas robust ANCOVA, as presented here, running interval smoothers.

The function \code{ancova} performs robust ANCOVA. In its current implementation it is limited to one factor with two categories and one 
covariate only. A bootstrap version of it is implemented as well (\code{ancboot}). Both functions perform the running interval smoothing on the trimmed means. Yuen tests for trimmed mean differences are performed at specified design points. It the design point argument (\code{pts}) is not specified, the routine picks five points automatically \citep[for details see][p. 611]{Wilcox:2012}. It is suggested that group sizes around the design point subject to Yuen's test should be at least 12. Regarding the multiple testing problem, the confidence intervals are adjusted to control the probability of at least one Type I error, the $p$-values are not.

The dataset we use to demonstrate robust ANCOVA is from \citet{Gelman+Hill:2007}. It is based on data involving an educational TV show for children called ``The Electric Company''. In each of four grades, the classes were randomized into treated groups and control groups. The kids in the treatment group were exposed to the TV show, those in the control group not. At the beginning and at the end of the school year, students in all the classes were given a reading test. The average test scores per class (pretest and posttest) were recorded. In this analysis we use the pretest score are covariate and are interested in possible differences between treatment and control group with respect to the postest scores. 
We are interested in comparisons at six particular design points. We set the smoothing parameters to a considerably small value.

<<echo=2:5>>=
comppts <- c(18, 70, 80, 90, 100, 110)
fitanc <- ancova(Posttest ~ Pretest + Group, fr1 = 0.3, fr2 = 0.3, 
                 data = electric, pts = comppts)
fitanc
@

Figure~\ref{fig:anc} shows the results of the robust ANCOVA fit. The vertical gray lines mark the design points. By taking into account 
the multiple testing nature of the problem, the only significant group difference we get for a pretest value of $x = 90$. For illustration, 
this plot also includes the linear regression fits for both subgroups (this is what a standard ANCOVA would do). 

<<anc-plot, eval=FALSE, echo=FALSE>>=
plot(electric$Pretest, electric$Posttest, col = rep(1:2, each = 96), pch = 1, cex = 0.8, 
      xlab = "Pretest Score", ylab = "Posttest Score", main = "TV Show ANCOVA")
eltr <- subset(electric, subset = Group == "treatment")
elct <- subset(electric, subset = Group == "control")
ordtr <- order(eltr$Pretest)
lines(eltr$Pretest[ordtr], fitanc$fitted.values$treatment[ordtr], col = 1, lwd = 2)
abline(lm(eltr$Posttest ~ eltr$Pretest), col = 1, lty = 2)
ordct <- order(elct$Pretest)
lines(elct$Pretest[ordct], fitanc$fitted.values$control[ordct], col = 2, lwd = 2)
abline(lm(elct$Posttest ~ elct$Pretest), col = 2, lty = 2)
abline(v = comppts, lty = 2, col = "gray")
legend(30, 120, legend = c("treatment", "control"), lty = 1, col = 1:2)
@
\begin{figure}[t]
\begin{center}  
<<anc-plot1, echo=FALSE, fig.height = 6, fig.width = 9, dev='postscript'>>=
<<anc-plot>>
@
\end{center}
\caption{\label{fig:anc} Robust ANCOVA fit on TV show data across treatment and control group. The nonparametric regression lines for 
both subgroups are shown as well as the OLS fit (dashed lines). The vertical lines show the design points our comparisons are based on.}
\end{figure}

\section{Discussion}
Future updates will include the following robust methods: mediator and moderator models, MANOVA, and intraclass correlation. In addition, 
functions for computing effect sizes will be available. 


%\section{Tests for Independence}
% 9.3

%\section{Robust Regression}
% Chapter 10-11
% mediator, moderator (11.7), ancova (11.11), longitudinal 11.12 

%\section{Robust Multivariate Methods}
% chapter 6

\bibliography{WRS2}
\end{document}


 
